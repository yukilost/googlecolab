{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningBasedFluids.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "16Xzv4ZiebTVDXGk42mqoWm02I5wbqEky",
      "authorship_tag": "ABX9TyM2EnpO0lLqAVv44fxdVHNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukilost/googlecolab/blob/main/LearningBasedFluids.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym3CKBQXa7KL"
      },
      "source": [
        "---\n",
        "TODO\n",
        "- ハイパーパラメータ管理\n",
        "- GPU設定\n",
        "- モデルの保存, 読み込み\n",
        "- 損失の視覚化\n",
        "- データセットの解凍の修正\n",
        "- 高速化\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoHoCZGuZn-6"
      },
      "source": [
        "---\n",
        "\n",
        "MEMO\n",
        "- 要素数が異なる場合はnp.ndarrayにできない\n",
        "- float32を使用する\n",
        "- MSEは2次元配列の各要素に対して適用されるため変更が必要\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVvlrPRt775i"
      },
      "source": [
        "# <strong> Graph Neural Networkを用いた非圧縮性流体の教師なし学習 </strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MamxZJF_snK"
      },
      "source": [
        "## 初期設定\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqMhCPzmA6By"
      },
      "source": [
        "ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6uc310FA_KX"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import neighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKEMEevBJr0"
      },
      "source": [
        "Google Driveのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UN-mpgeBMbg",
        "outputId": "cf0b6eb4-8799-4d83-e63b-a274a57537f3"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p1fAiboBRYV"
      },
      "source": [
        "デバイスの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8z_CqmmBW1R",
        "outputId": "ffb876e9-0130-4e2e-e85c-0d9048a8ba7d"
      },
      "source": [
        "device: torch.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device: {}\".format(device))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pshKp7QILyMr"
      },
      "source": [
        "## Utility\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P95u0wepLuzq"
      },
      "source": [
        "def parse_vector(v):\n",
        "  \"\"\"\n",
        "  @fn parse_vector\n",
        "  @brief 辞書型のベクトルのパース\n",
        "  @param v[dict] 辞書型のベクトル\n",
        "  @return [np.ndarray] ベクトル\n",
        "  \"\"\"\n",
        "  return np.array([v[\"x\"], v[\"y\"], v[\"z\"]])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWFdvNvY105B"
      },
      "source": [
        "def choice(arr, n):\n",
        "  \"\"\"\n",
        "  @fn choice\n",
        "  @brief 要素をランダムに抽出する\n",
        "  @param arr[np.ndarray] 抽出対象とする配列\n",
        "  @param n[int] 抽出する数\n",
        "  @retval [np.ndarray] 抽出した配列\n",
        "  @retval [np.ndarray] 元の配列から抽出した数を除いた配列\n",
        "  \"\"\"\n",
        "  s = np.random.choice(arr, n, replace=False)\n",
        "  arr = np.setdiff1d(arr, s)\n",
        "  return s, arr"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DAYVBkwuO3"
      },
      "source": [
        "def split(n, n_train, n_valid, n_test):\n",
        "  \"\"\"\n",
        "  @fn split_data\n",
        "  @brief インデックスをtrain, valid, testに分割する\n",
        "  @param n[int] インデックス数\n",
        "  @param n_train[int] トレーニングデータ数\n",
        "  @param n_valid[int] バリデーションデータ数\n",
        "  @param n_test[int] テストデータ数\n",
        "  @retval [np.ndarray] トレーニングデータに使用するインデックス\n",
        "  @retval [np.ndarray] バリデーションデータに使用するインデックス\n",
        "  @retval [np.ndarray] テストデータに使用するインデックス\n",
        "  \"\"\"\n",
        "  assert n >= n_train + n_valid + n_test, \"Data is not split correctly.\"\n",
        "  s = np.arange(1, n+1) \n",
        "  s_train, s = choice(s, n_train)\n",
        "  s_valid, s = choice(s, n_valid)\n",
        "  s_test, s = choice(s, n_test)\n",
        "  return s_train, s_valid, s_test"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51xj1BJo6K9R"
      },
      "source": [
        "class Simulation():\n",
        "  \"\"\"\n",
        "  @class Simulation\n",
        "  @brief シミュレーションで使用する変数の設定\n",
        "  \"\"\"\n",
        "  def __init__(self, path_config):\n",
        "    \"\"\"\n",
        "    @fn __init__\n",
        "    @brief コンストラクタ\n",
        "    @param path_config[str] 設定ファイルのパス\n",
        "    \"\"\"\n",
        "    # 設定ファイル読み込み\n",
        "    config = pd.read_json(path_config)\n",
        "\n",
        "    # 粒子数\n",
        "    self.boundary_size = parse_vector(config[\"Simulation\"][\"BoundarySize\"])\n",
        "    self.num_boundary_particles = self.boundary_size.prod() - (self.boundary_size-2).prod()\n",
        "    self.fluid_size = parse_vector(config[\"Simulation\"][\"FluidSize\"])\n",
        "    self.num_fluid_particles = self.fluid_size.prod()\n",
        "    self.num_particles = self.num_boundary_particles + self.num_fluid_particles\n",
        "\n",
        "    # 流体\n",
        "    self.particle_mass = config[\"Simulation\"][\"ParticleMass\"]\n",
        "    self.rest_density = config[\"Simulation\"][\"RestDensity\"]\n",
        "    self.kernel_particles = config[\"Simulation\"][\"KernelParticles\"]\n",
        "    self.volume = self.num_fluid_particles * self.particle_mass / self.rest_density;\n",
        "\n",
        "    # 半径\n",
        "    self.effective_radius = pow(((3.0*self.kernel_particles*self.volume)/(4.0*self.num_fluid_particles*math.pi)), 1.0/3.0);\n",
        "    self.kernel_radius = self.effective_radius\n",
        "    self.particle_radius = pow((math.pi/(6.0*self.kernel_particles)), 1.0/3.0)*self.effective_radius;\n",
        "\n",
        "    # 境界\n",
        "    self.wall_min = np.array(2.0*self.particle_radius*np.ones(3))\n",
        "    self.wall_max = np.array(2.0*self.particle_radius*(self.boundary_size-1))\n",
        "\n",
        "    # タイムステップ\n",
        "    self.timesteps = config[\"Simulation\"][\"MaxTimestep\"]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVE4Px_szYoh"
      },
      "source": [
        "# kd-treeを用いた近傍粒子探索の実装\n",
        "# def NearestNeighborSearch(pos: np.ndarray, radius: float):\n",
        "#   tree: neighbors.KDTree = neighbors.KDTree(pos, leaf_size=2)\n",
        "#   idx: np.ndarray = tree.query_radius(pos[:], r=radius)\n",
        "#   return idx"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5-tp1VKSXo"
      },
      "source": [
        "class NearestNeighborSearch():\n",
        "  \"\"\"\n",
        "  @class NearestNeighborSearch\n",
        "  @brief 近傍粒子探索\n",
        "  \"\"\"\n",
        "  def __init__(self, sim, radius):\n",
        "    \"\"\"\n",
        "    @fn __init__\n",
        "    @brief コンストラクタ\n",
        "    @param sim[Simulation] シミュレーション管理変数\n",
        "    @param radius[float] 有効半径\n",
        "    \"\"\"\n",
        "    self.radius = radius\n",
        "    self.grid_num = np.ceil(sim.boundary_size * 2.0 * sim.particle_radius // self.radius).astype(int) + 1\n",
        "    self.grid_num_all = int(self.grid_num.prod())\n",
        "    self.pos_min = np.zeros(3)\n",
        "    self.pos_max = 2.0*sim.particle_radius*(sim.boundary_size+1)\n",
        "    self.hash_index = [0] * sim.num_particles\n",
        "    self.cell_start = np.zeros((self.grid_num_all), dtype=np.int)\n",
        "    self.cell_end = np.zeros((self.grid_num_all), dtype=np.int)\n",
        "\n",
        "  def IsValid(self, pos):\n",
        "    \"\"\"\n",
        "    @fn IsValid\n",
        "    @brief ハッシュ値が有効かどうか判断する\n",
        "    @param pos[np.ndarray] 粒子位置\n",
        "    @return [bool] 有効かどうか\n",
        "    \"\"\"\n",
        "    return all(pos >= self.pos_min) and all(pos < self.pos_max)\n",
        "\n",
        "  def CalcHash(self, pos):\n",
        "    \"\"\"\n",
        "    @fn CalcHash\n",
        "    @brief ハッシュ値を計算する\n",
        "    @param pos[np.ndarray] 粒子位置\n",
        "    @return [int] ハッシュ値\n",
        "    \"\"\"\n",
        "    r = self.radius\n",
        "\n",
        "    if not self.IsValid(pos):\n",
        "      return -1\n",
        "\n",
        "    p = pos // r\n",
        "    hash = p[0] + self.grid_num[0]*p[1] + self.grid_num[0]*self.grid_num[1]*p[2]\n",
        "    return hash.astype(np.int)\n",
        "\n",
        "  def NeighborGrid(self, pos):\n",
        "    \"\"\"\n",
        "    @fn NighborGrid\n",
        "    @brief 近傍格子を計算する\n",
        "    @param pos[np.ndarray] 粒子位置\n",
        "    @return [np.ndarray] 近傍格子のハッシュ値\n",
        "    \"\"\"\n",
        "    neighbor_hash = []\n",
        "    for dx in range(-1, 2):\n",
        "      for dy in range(-1, 2):\n",
        "        for dz in range(-1, 2):\n",
        "          neighbor_pos = pos + self.radius*np.array([dx, dy, dz])\n",
        "          hash = self.CalcHash(neighbor_pos) \n",
        "          if hash == -1:\n",
        "            continue\n",
        "          neighbor_hash.append(hash)\n",
        "    return np.array(neighbor_hash, dtype=np.int)\n",
        "\n",
        "  def Set(self, pos):\n",
        "    \"\"\"\n",
        "    @fn Set\n",
        "    @brief 粒子を格子に登録する\n",
        "    @param pos[np.ndarray] 粒子位置\n",
        "    \"\"\"\n",
        "    n = pos.shape[0]\n",
        "    self.hash_index = [0]*n\n",
        "    for i, p in enumerate(pos):\n",
        "      hash = self.CalcHash(p)\n",
        "      self.hash_index[i] = (hash, i)\n",
        "    sorted_hash_index = np.array(sorted(self.hash_index))\n",
        "    self.hash_index = sorted_hash_index\n",
        "\n",
        "    self.cell_start[:] = 0xffffffff\n",
        "    self.cell_end[:] = 0xffffffff\n",
        "\n",
        "    # ハッシュ値が0以上の場所\n",
        "    if self.hash_index[n-1][0] == -1:\n",
        "      return\n",
        "    idx = self.hash_index > (-1, -1)\n",
        "    idx = np.all(idx, axis=1)\n",
        "    idx = np.where(idx)[0][0]\n",
        "\n",
        "    # 特定ハッシュ値の始点と終点を記録\n",
        "    self.cell_start[self.hash_index[idx][0].astype(np.int)] = idx\n",
        "    for i in range(idx+1, n):\n",
        "      if self.hash_index[i][0] != self.hash_index[i-1][0]:\n",
        "        self.cell_end[self.hash_index[i-1][0].astype(np.int)] = i\n",
        "        self.cell_start[self.hash_index[i][0].astype(np.int)] = i\n",
        "    self.cell_end[self.hash_index[n-1][0].astype(np.int)] = n\n",
        "\n",
        "  def Search(self, pos):\n",
        "    \"\"\"\n",
        "    @fn Search\n",
        "    @brief 近傍粒子探索を行う\n",
        "    @param pos[np.ndarray] 粒子位置\n",
        "    @return [np.ndarray] 近傍粒子のインデックス\n",
        "    \"\"\"\n",
        "    n = pos.shape[0]\n",
        "    res = []\n",
        "    for p in pos:\n",
        "      adj = []\n",
        "      neighbor_grid = self.NeighborGrid(p)\n",
        "      # 近傍格子のハッシュ値\n",
        "      for hash in neighbor_grid:\n",
        "        for i in range(self.cell_start[hash].astype(np.int), self.cell_end[hash].astype(np.int)):\n",
        "          # 近傍格子内に存在する粒子のインデックス\n",
        "          idx = self.hash_index[i][1].astype(np.int)\n",
        "          d = np.linalg.norm(p-pos[idx])\n",
        "          if d <= self.radius:\n",
        "            adj.append(idx)\n",
        "      res.append(np.array(adj))\n",
        "    return np.array(res, dtype=object)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tWPNu0UAsM5"
      },
      "source": [
        "class Graph():\n",
        "  \"\"\"\n",
        "  @class Graph\n",
        "  @brief グラフ構造\n",
        "  \"\"\"\n",
        "  def __init__(self, v, e, g):\n",
        "    \"\"\"\n",
        "    @fn __init__\n",
        "    @brief コンストラクタ\n",
        "    @param v[torch.tensor] 頂点の特徴量\n",
        "    @param e[torch.tensor] 辺の特徴量\n",
        "    @param g[torch.tensor] グローバル特徴量\n",
        "    \"\"\"\n",
        "    self.v = v\n",
        "    self.e = e\n",
        "    self.g = g\n",
        "  \n",
        "  def __iadd__(self, other):\n",
        "    \"\"\"\n",
        "    @fn __iadd__\n",
        "    @brief +=のオーバロード\n",
        "    @param other[Graph] グラフ\n",
        "    @return [Graph] グラフ\n",
        "    \"\"\"\n",
        "    self.v += other.v\n",
        "    self.e += other.e\n",
        "    self.g += other.g\n",
        "    return self"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KVZyuouQnA1"
      },
      "source": [
        "class FluidInfo():\n",
        "  \"\"\"\n",
        "  @class FluidInfo\n",
        "  @brief 流体の状態\n",
        "  \"\"\"\n",
        "  def __init__(self, df):\n",
        "    \"\"\"\n",
        "    @fn __init__\n",
        "    @brief コンストラクタ\n",
        "    @param df 特定のタイムステップのデータ\n",
        "    \"\"\"\n",
        "    self.pos = df[[\"position.x\", \"position.y\", \"position.z\"]].values\n",
        "    self.vel = df[[\"velocity.x\", \"velocity.y\", \"velocity.z\"]].values\n",
        "    self.acc = df[[\"acc.x\", \"acc.y\", \"acc.z\"]].values\n",
        "    self.vol = df[[\"volume\"]].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxqdX791akGb"
      },
      "source": [
        "def MatchEdge(idx):\n",
        "  \"\"\"\n",
        "  @fn MatchEdge\n",
        "  @brief エッジの対応付けを行う\n",
        "  @param idx[np.ndarray] 近傍粒子のインデックス\n",
        "  @retval [np.ndarray] 入頂点\n",
        "  @retval [np.ndarray] 出頂点\n",
        "  \"\"\"\n",
        "  receivers = np.zeros(0, dtype=np.int) \n",
        "  senders = np.zeros(0, dtype=np.int)\n",
        "  for receiver, neighbor in enumerate(idx):\n",
        "    for sender in neighbor:\n",
        "      if sender == receiver: \n",
        "        continue\n",
        "      receivers = np.append(receivers, receiver)\n",
        "      senders = np.append(senders, sender)\n",
        "  return receivers, senders"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrNOamtD_0ii"
      },
      "source": [
        "## データセット\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYycC-D9Bhrd"
      },
      "source": [
        "データセットの解凍"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKckC3aGBg2A",
        "outputId": "2982a0c5-6f67-42a5-f1e0-6733bd032648"
      },
      "source": [
        "!unzip /content/drive/MyDrive/LearningBasedFluids/data.zip -d /content/graph_neural_network/\n",
        "\n",
        "# 出力消去\n",
        "output.clear()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/LearningBasedFluids/data.zip\n",
            "   creating: /content/graph_neural_network/data/\n",
            "   creating: /content/graph_neural_network/data/20/\n",
            "  inflating: /content/graph_neural_network/data/20/20.json  \n",
            "  inflating: /content/graph_neural_network/data/20/20.csv  \n",
            "   creating: /content/graph_neural_network/data/18/\n",
            "  inflating: /content/graph_neural_network/data/18/18.csv  \n",
            "  inflating: /content/graph_neural_network/data/18/18.json  \n",
            "   creating: /content/graph_neural_network/data/27/\n",
            "  inflating: /content/graph_neural_network/data/27/27.json  \n",
            "  inflating: /content/graph_neural_network/data/27/27.csv  \n",
            "   creating: /content/graph_neural_network/data/9/\n",
            "  inflating: /content/graph_neural_network/data/9/9.csv  \n",
            "  inflating: /content/graph_neural_network/data/9/9.json  \n",
            "   creating: /content/graph_neural_network/data/11/\n",
            "  inflating: /content/graph_neural_network/data/11/11.json  \n",
            "  inflating: /content/graph_neural_network/data/11/11.csv  \n",
            "   creating: /content/graph_neural_network/data/7/\n",
            "  inflating: /content/graph_neural_network/data/7/7.csv  \n",
            "  inflating: /content/graph_neural_network/data/7/7.json  \n",
            "   creating: /content/graph_neural_network/data/29/\n",
            "  inflating: /content/graph_neural_network/data/29/29.csv  \n",
            "  inflating: /content/graph_neural_network/data/29/29.json  \n",
            "   creating: /content/graph_neural_network/data/16/\n",
            "  inflating: /content/graph_neural_network/data/16/16.json  \n",
            "  inflating: /content/graph_neural_network/data/16/16.csv  \n",
            "   creating: /content/graph_neural_network/data/6/\n",
            "  inflating: /content/graph_neural_network/data/6/6.csv  \n",
            "  inflating: /content/graph_neural_network/data/6/6.json  \n",
            "   creating: /content/graph_neural_network/data/28/\n",
            "  inflating: /content/graph_neural_network/data/28/28.json  \n",
            "  inflating: /content/graph_neural_network/data/28/28.csv  \n",
            "   creating: /content/graph_neural_network/data/17/\n",
            "  inflating: /content/graph_neural_network/data/17/17.json  \n",
            "  inflating: /content/graph_neural_network/data/17/17.csv  \n",
            "   creating: /content/graph_neural_network/data/1/\n",
            "  inflating: /content/graph_neural_network/data/1/1.csv  \n",
            "  inflating: /content/graph_neural_network/data/1/1.json  \n",
            "   creating: /content/graph_neural_network/data/10/\n",
            "  inflating: /content/graph_neural_network/data/10/10.json  \n",
            "  inflating: /content/graph_neural_network/data/10/10.csv  \n",
            "   creating: /content/graph_neural_network/data/19/\n",
            "  inflating: /content/graph_neural_network/data/19/19.csv  \n",
            "  inflating: /content/graph_neural_network/data/19/19.json  \n",
            "   creating: /content/graph_neural_network/data/26/\n",
            "  inflating: /content/graph_neural_network/data/26/26.json  \n",
            "  inflating: /content/graph_neural_network/data/26/26.csv  \n",
            "   creating: /content/graph_neural_network/data/8/\n",
            "  inflating: /content/graph_neural_network/data/8/8.json  \n",
            "  inflating: /content/graph_neural_network/data/8/8.csv  \n",
            "   creating: /content/graph_neural_network/data/21/\n",
            "  inflating: /content/graph_neural_network/data/21/21.json  \n",
            "  inflating: /content/graph_neural_network/data/21/21.csv  \n",
            "   creating: /content/graph_neural_network/data/30/\n",
            "  inflating: /content/graph_neural_network/data/30/30.json  \n",
            "  inflating: /content/graph_neural_network/data/30/30.csv  \n",
            "   creating: /content/graph_neural_network/data/24/\n",
            "  inflating: /content/graph_neural_network/data/24/24.csv  \n",
            "  inflating: /content/graph_neural_network/data/24/24.json  \n",
            "   creating: /content/graph_neural_network/data/23/\n",
            "  inflating: /content/graph_neural_network/data/23/23.csv  \n",
            "  inflating: /content/graph_neural_network/data/23/23.json  \n",
            "   creating: /content/graph_neural_network/data/4/\n",
            "  inflating: /content/graph_neural_network/data/4/4.csv  \n",
            "  inflating: /content/graph_neural_network/data/4/4.json  \n",
            "   creating: /content/graph_neural_network/data/15/\n",
            "  inflating: /content/graph_neural_network/data/15/15.csv  \n",
            "  inflating: /content/graph_neural_network/data/15/15.json  \n",
            "   creating: /content/graph_neural_network/data/3/\n",
            "  inflating: /content/graph_neural_network/data/3/3.csv  \n",
            "  inflating: /content/graph_neural_network/data/3/3.json  \n",
            "   creating: /content/graph_neural_network/data/12/\n",
            "  inflating: /content/graph_neural_network/data/12/12.json  \n",
            "  inflating: /content/graph_neural_network/data/12/12.csv  \n",
            "   creating: /content/graph_neural_network/data/2/\n",
            "  inflating: /content/graph_neural_network/data/2/2.csv  \n",
            "  inflating: /content/graph_neural_network/data/2/2.json  \n",
            "   creating: /content/graph_neural_network/data/13/\n",
            "  inflating: /content/graph_neural_network/data/13/13.json  \n",
            "  inflating: /content/graph_neural_network/data/13/13.csv  \n",
            "   creating: /content/graph_neural_network/data/5/\n",
            "  inflating: /content/graph_neural_network/data/5/5.csv  \n",
            "  inflating: /content/graph_neural_network/data/5/5.json  \n",
            "   creating: /content/graph_neural_network/data/14/\n",
            "  inflating: /content/graph_neural_network/data/14/14.csv  \n",
            "  inflating: /content/graph_neural_network/data/14/14.json  \n",
            "   creating: /content/graph_neural_network/data/22/\n",
            "  inflating: /content/graph_neural_network/data/22/22.csv  \n",
            "  inflating: /content/graph_neural_network/data/22/22.json  \n",
            "   creating: /content/graph_neural_network/data/25/\n",
            "  inflating: /content/graph_neural_network/data/25/25.csv  \n",
            "  inflating: /content/graph_neural_network/data/25/25.json  \n",
            "   creating: /content/graph_neural_network/processed/\n",
            "   creating: /content/graph_neural_network/processed/20/\n",
            "  inflating: /content/graph_neural_network/processed/20/20.csv  \n",
            "   creating: /content/graph_neural_network/processed/18/\n",
            "  inflating: /content/graph_neural_network/processed/18/18.csv  \n",
            "   creating: /content/graph_neural_network/processed/27/\n",
            "  inflating: /content/graph_neural_network/processed/27/27.csv  \n",
            "   creating: /content/graph_neural_network/processed/9/\n",
            "  inflating: /content/graph_neural_network/processed/9/9.csv  \n",
            "   creating: /content/graph_neural_network/processed/11/\n",
            "  inflating: /content/graph_neural_network/processed/11/11.csv  \n",
            "   creating: /content/graph_neural_network/processed/7/\n",
            "  inflating: /content/graph_neural_network/processed/7/7.csv  \n",
            "   creating: /content/graph_neural_network/processed/29/\n",
            "  inflating: /content/graph_neural_network/processed/29/29.csv  \n",
            "   creating: /content/graph_neural_network/processed/16/\n",
            "  inflating: /content/graph_neural_network/processed/16/16.csv  \n",
            "   creating: /content/graph_neural_network/processed/6/\n",
            "  inflating: /content/graph_neural_network/processed/6/6.csv  \n",
            "   creating: /content/graph_neural_network/processed/28/\n",
            "  inflating: /content/graph_neural_network/processed/28/28.csv  \n",
            "   creating: /content/graph_neural_network/processed/17/\n",
            "  inflating: /content/graph_neural_network/processed/17/17.csv  \n",
            "   creating: /content/graph_neural_network/processed/1/\n",
            "  inflating: /content/graph_neural_network/processed/1/1.csv  \n",
            "   creating: /content/graph_neural_network/processed/10/\n",
            "  inflating: /content/graph_neural_network/processed/10/10.csv  \n",
            "   creating: /content/graph_neural_network/processed/19/\n",
            "  inflating: /content/graph_neural_network/processed/19/19.csv  \n",
            "   creating: /content/graph_neural_network/processed/26/\n",
            "  inflating: /content/graph_neural_network/processed/26/26.csv  \n",
            "   creating: /content/graph_neural_network/processed/8/\n",
            "  inflating: /content/graph_neural_network/processed/8/8.csv  \n",
            "   creating: /content/graph_neural_network/processed/21/\n",
            "  inflating: /content/graph_neural_network/processed/21/21.csv  \n",
            "   creating: /content/graph_neural_network/processed/30/\n",
            "  inflating: /content/graph_neural_network/processed/30/30.csv  \n",
            "   creating: /content/graph_neural_network/processed/24/\n",
            "  inflating: /content/graph_neural_network/processed/24/24.csv  \n",
            "   creating: /content/graph_neural_network/processed/23/\n",
            "  inflating: /content/graph_neural_network/processed/23/23.csv  \n",
            "   creating: /content/graph_neural_network/processed/4/\n",
            "  inflating: /content/graph_neural_network/processed/4/4.csv  \n",
            "   creating: /content/graph_neural_network/processed/15/\n",
            "  inflating: /content/graph_neural_network/processed/15/15.csv  \n",
            "   creating: /content/graph_neural_network/processed/3/\n",
            "  inflating: /content/graph_neural_network/processed/3/3.csv  \n",
            "   creating: /content/graph_neural_network/processed/12/\n",
            "  inflating: /content/graph_neural_network/processed/12/12.csv  \n",
            "   creating: /content/graph_neural_network/processed/2/\n",
            "  inflating: /content/graph_neural_network/processed/2/2.csv  \n",
            "   creating: /content/graph_neural_network/processed/13/\n",
            "  inflating: /content/graph_neural_network/processed/13/13.csv  \n",
            "   creating: /content/graph_neural_network/processed/5/\n",
            "  inflating: /content/graph_neural_network/processed/5/5.csv  \n",
            "   creating: /content/graph_neural_network/processed/14/\n",
            "  inflating: /content/graph_neural_network/processed/14/14.csv  \n",
            "   creating: /content/graph_neural_network/processed/22/\n",
            "  inflating: /content/graph_neural_network/processed/22/22.csv  \n",
            "   creating: /content/graph_neural_network/processed/25/\n",
            "  inflating: /content/graph_neural_network/processed/25/25.csv  \n",
            "  inflating: /content/graph_neural_network/processed/20/20.json  \n",
            "  inflating: /content/graph_neural_network/processed/18/18.json  \n",
            "  inflating: /content/graph_neural_network/processed/27/27.json  \n",
            "  inflating: /content/graph_neural_network/processed/9/9.json  \n",
            "  inflating: /content/graph_neural_network/processed/11/11.json  \n",
            "  inflating: /content/graph_neural_network/processed/7/7.json  \n",
            "  inflating: /content/graph_neural_network/processed/29/29.json  \n",
            "  inflating: /content/graph_neural_network/processed/16/16.json  \n",
            "  inflating: /content/graph_neural_network/processed/6/6.json  \n",
            "  inflating: /content/graph_neural_network/processed/28/28.json  \n",
            "  inflating: /content/graph_neural_network/processed/17/17.json  \n",
            "  inflating: /content/graph_neural_network/processed/1/1.json  \n",
            "  inflating: /content/graph_neural_network/processed/10/10.json  \n",
            "  inflating: /content/graph_neural_network/processed/19/19.json  \n",
            "  inflating: /content/graph_neural_network/processed/26/26.json  \n",
            "  inflating: /content/graph_neural_network/processed/8/8.json  \n",
            "  inflating: /content/graph_neural_network/processed/21/21.json  \n",
            "  inflating: /content/graph_neural_network/processed/30/30.json  \n",
            "  inflating: /content/graph_neural_network/processed/24/24.json  \n",
            "  inflating: /content/graph_neural_network/processed/23/23.json  \n",
            "  inflating: /content/graph_neural_network/processed/4/4.json  \n",
            "  inflating: /content/graph_neural_network/processed/15/15.json  \n",
            "  inflating: /content/graph_neural_network/processed/3/3.json  \n",
            "  inflating: /content/graph_neural_network/processed/12/12.json  \n",
            "  inflating: /content/graph_neural_network/processed/2/2.json  \n",
            "  inflating: /content/graph_neural_network/processed/13/13.json  \n",
            "  inflating: /content/graph_neural_network/processed/5/5.json  \n",
            "  inflating: /content/graph_neural_network/processed/14/14.json  \n",
            "  inflating: /content/graph_neural_network/processed/22/22.json  \n",
            "  inflating: /content/graph_neural_network/processed/25/25.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GClOGlDmW01t"
      },
      "source": [
        "## ネットワーク\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nk6FWrdqJ59"
      },
      "source": [
        "dim = 128\n",
        "\n",
        "class GraphNeuralNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  @class GraphNeuralNetwork\n",
        "  @brief グラフニューラルネットワーク\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    @fn __init__\n",
        "    @brief コンストラクタ\n",
        "    \"\"\"\n",
        "    super().__init__() \n",
        "\n",
        "    # Encoder\n",
        "    self.epsilon_v = nn.Sequential(\n",
        "        nn.Linear(in_features=10, out_features=dim), nn.ReLU(), \n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(),\n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU()\n",
        "    )\n",
        "    self.epsilon_e = nn.Sequential(\n",
        "        nn.Linear(in_features=4, out_features=dim), nn.ReLU(), \n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(),\n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    # Processor\n",
        "    self.phi_v = nn.Sequential(\n",
        "        nn.Linear(in_features=dim*2+3, out_features=dim), nn.ReLU(), \n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(),\n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU()\n",
        "    )\n",
        "    self.phi_e = nn.Sequential(\n",
        "        nn.Linear(in_features=dim*3, out_features=dim), nn.ReLU(), \n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(),\n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # Decoder\n",
        "    self.delta_v = nn.Sequential(\n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(), \n",
        "        nn.Linear(in_features=dim, out_features=dim), nn.ReLU(), \n",
        "        nn.Linear(in_features=dim, out_features=3)\n",
        "    )\n",
        "\n",
        "  def forward(self, sim, info, nns):\n",
        "    \"\"\"\n",
        "    @fn forward\n",
        "    @brief 推論\n",
        "    @param sim[Simulation] シミュレーション環境\n",
        "    @param info[FluidInfo] 流体の状態\n",
        "    @return [torch.tensor] 加速度\n",
        "    \"\"\"\n",
        "    graph, receivers, senders = self.encoder(sim, info, nns)\n",
        "    graph += self.processor(graph, receivers, senders)\n",
        "    out = self.decoder(graph)\n",
        "    return out\n",
        "\n",
        "  def encoder(self, sim, info, nns):\n",
        "    \"\"\"\n",
        "    @fn encoder\n",
        "    @brief グラフの構築\n",
        "    @param sim[Simulation] シミュレーション環境\n",
        "    @param info[FluidInfo] 流体の状態\n",
        "    @retval [Graph] 潜在空間上のグラフ\n",
        "    @retval [np.ndarray] 入頂点\n",
        "    @retval [np.ndarray] 出頂点\n",
        "    \"\"\"\n",
        "    # 近傍粒子探索\n",
        "    nns.Set(info.pos)\n",
        "    idx = nns.Search(info.pos)\n",
        "    receivers, senders = MatchEdge(idx)\n",
        "\n",
        "    # 頂点の特徴量\n",
        "    wall_dist_min = abs(sim.wall_min - info.pos)\n",
        "    wall_dist_max = abs(sim.wall_max - info.pos)\n",
        "\n",
        "    v = np.concatenate((info.vel, info.vol, wall_dist_min, wall_dist_max), axis=1).astype(np.float32)\n",
        "    v = torch.from_numpy(v).clone()\n",
        "    v = self.epsilon_v(v)\n",
        "  \n",
        "    # 辺の特徴量\n",
        "    relative_pos = info.pos[senders] - info.pos[receivers]\n",
        "    magnitude = np.linalg.norm(relative_pos, axis=1).reshape(-1, 1)\n",
        "    e = np.hstack((relative_pos, magnitude)).astype(np.float32)\n",
        "    e = torch.from_numpy(e).clone()\n",
        "    e = self.epsilon_e(e)\n",
        "\n",
        "    # グラフの特徴量\n",
        "    g = np.array([0.0, -9.8, 0.0], dtype=np.float32)\n",
        "    g = torch.from_numpy(g).clone()\n",
        "\n",
        "    # グラフ構築\n",
        "    graph = Graph(v, e, g)\n",
        "\n",
        "    return graph, receivers, senders\n",
        "\n",
        "  def processor(self, graph, receivers, senders):\n",
        "    \"\"\"\n",
        "    @fn processor\n",
        "    @brief 特徴量の更新\n",
        "    @param graph[Graph] 潜在空間上のグラフ\n",
        "    @param receivers[np.ndarray] 入頂点\n",
        "    @param senders[np.ndarray] 出頂点\n",
        "    @retval [Graph] グラフ\n",
        "    \"\"\"\n",
        "    # 辺の更新\n",
        "    e = torch.cat((graph.e, graph.v[receivers], graph.v[senders]), axis=1)\n",
        "    e = self.phi_e(e)\n",
        "    \n",
        "    # 頂点の更新\n",
        "    agg_e = self.aggregate(receivers, graph.e, graph.v.shape)\n",
        "    glo = graph.g.expand((graph.v.shape[0], graph.g.shape[0]))\n",
        "    v = torch.cat((graph.v, agg_e, glo), axis=1)\n",
        "    v = self.phi_v(v)\n",
        "\n",
        "    # グローバル特徴量の更新\n",
        "    g = torch.zeros(3)\n",
        "\n",
        "    return Graph(v, e, g)\n",
        "\n",
        "  def decoder(self, graph):\n",
        "    \"\"\"\n",
        "    @fn decoder\n",
        "    @brief 出力の抽出\n",
        "    @param graph[Graph] 潜在空間上のグラフ\n",
        "    @return [torch.tensor] 加速度\n",
        "    \"\"\"\n",
        "    out = self.delta_v(graph.v)\n",
        "    return out\n",
        "\n",
        "  def aggregate(self, receivers, e, shape):\n",
        "    \"\"\"\n",
        "    @fn aggregate\n",
        "    @brief 特徴量の集約\n",
        "    @param receivers[np.ndarray] 入頂点\n",
        "    @param e[torch.tensor] 辺の特徴量\n",
        "    @param shape[tuple] 頂点数と次元\n",
        "    @return [torch.tensor] 集約した特徴量\n",
        "    \"\"\"\n",
        "    val = torch.zeros(shape)\n",
        "    for i in range(shape[0]):\n",
        "      agg = e[receivers==i]\n",
        "      val[i] = torch.mean(agg, axis=0)\n",
        "    return val"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu_jn6aQIm4w"
      },
      "source": [
        "## 学習\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml6auz3n5ayT"
      },
      "source": [
        "path = \"/content/graph_neural_network/processed/\"\n",
        "criterion = nn.MSELoss()\n",
        "model = GraphNeuralNetwork()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# データセット分割\n",
        "train_scenes, valid_scenes, test_scenes = split(30, 20, 5, 5)\n",
        "\n",
        "def train(model):\n",
        "  \"\"\"\n",
        "  @fn train\n",
        "  @brief トレーニング\n",
        "  @param model[GraphNeuralNetwork] 学習するモデル\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  for scene in train_scenes:\n",
        "    print(\"scene: {}\".format(scene))\n",
        "    # 設定ファイル読み込み\n",
        "    sim = Simulation(path+\"{}/{}.json\".format(scene, scene))\n",
        "    # シーン読み込み\n",
        "    df = pd.read_csv(path+\"{}/{}.csv\".format(scene, scene))\n",
        "    # 流体粒子ラベルの作成\n",
        "    label_fluid = np.array([False]*(sim.num_particles-sim.num_fluid_particles)+[True]*(sim.num_fluid_particles))\n",
        "    # 学習用パラメータの設定, Rなど?\n",
        "    radius = 0.08\n",
        "    # 近傍粒子探索の設定\n",
        "    nns = NearestNeighborSearch(sim, radius)\n",
        "    for timestep in range(1, sim.timesteps):\n",
        "      print(\"timestep: {}\".format(timestep))\n",
        "      optimizer.zero_grad()\n",
        "      dft = df[df[\"timestep\"]==timestep]\n",
        "      info = FluidInfo(dft)\n",
        "      y = torch.tensor(info.acc[label_fluid]).float()\n",
        "      pred = model(sim, info, nns)[label_fluid]\n",
        "      loss = 3*criterion(y, pred) # MSELossの次元の都合で3をかける\n",
        "      print(\"loss: {}\".format(loss.item())) \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "def valid(model):\n",
        "  \"\"\"\n",
        "  @fn valid\n",
        "  @brief バリデーション\n",
        "  @param model[GraphNeuralNetwork] 評価するモデル\n",
        "  \"\"\"\n",
        "  model.valid()\n",
        "  with torch.no_grad():\n",
        "    for scene in valid_scenes:\n",
        "      # 設定ファイル読み込み\n",
        "      sim: Simulation = Simulation(path+\"{}/{}.json\".format(scene, scene))\n",
        "      # シーン読み込み\n",
        "      df: pd.DataFrame = pd.read_csv(path+\"{}/{}.csv\".format(scene, scene))\n",
        "      # 流体粒子ラベルの作成\n",
        "      label_fluid = df[df[\"timestep\"]==1][\"label\"].values == 1\n",
        "      # 学習用パラメータの設定, Rなど?\n",
        "      for timestep in range(sim.timesteps):\n",
        "        optimizer.zero_grad()\n",
        "        dft = df[df[\"timestep\"]==timestep]\n",
        "        acc = model()\n",
        "        loss = criterion()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSAPhbsz4fXz",
        "outputId": "d4eb6a7e-b228-4088-bd7a-6f55888c90c2"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"-\"*50)\n",
        "  print(\"epoch: {}/{}\".format(epoch+1, epochs))\n",
        "  train(model)\n",
        "  #valid(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "epoch: 1/10\n",
            "scene: 6\n",
            "timestep: 1\n",
            "loss: 105.21780395507812\n",
            "timestep: 2\n",
            "loss: 98.59842681884766\n",
            "timestep: 3\n",
            "loss: 94.06672668457031\n",
            "timestep: 4\n",
            "loss: 93.6860122680664\n",
            "timestep: 5\n",
            "loss: 94.85629272460938\n",
            "timestep: 6\n",
            "loss: 93.76306915283203\n",
            "timestep: 7\n",
            "loss: 90.25790405273438\n",
            "timestep: 8\n",
            "loss: 86.13849639892578\n",
            "timestep: 9\n",
            "loss: 82.12525939941406\n",
            "timestep: 10\n",
            "loss: 77.6533432006836\n",
            "timestep: 11\n",
            "loss: 75.87627410888672\n",
            "timestep: 12\n",
            "loss: 665.7382202148438\n",
            "timestep: 13\n",
            "loss: 418.6941833496094\n",
            "timestep: 14\n",
            "loss: 778.4749755859375\n",
            "timestep: 15\n",
            "loss: 828.086669921875\n",
            "timestep: 16\n",
            "loss: 702.0244750976562\n",
            "timestep: 17\n",
            "loss: 674.5125732421875\n",
            "timestep: 18\n",
            "loss: 465.3511047363281\n",
            "timestep: 19\n",
            "loss: 417.28717041015625\n",
            "timestep: 20\n",
            "loss: 393.78656005859375\n",
            "timestep: 21\n",
            "loss: 659.0706176757812\n",
            "timestep: 22\n",
            "loss: 868.3950805664062\n",
            "timestep: 23\n",
            "loss: 1512.86767578125\n",
            "timestep: 24\n",
            "loss: 1314.1661376953125\n",
            "timestep: 25\n",
            "loss: 1377.125244140625\n",
            "timestep: 26\n",
            "loss: 1208.33251953125\n",
            "timestep: 27\n",
            "loss: 1083.911865234375\n",
            "timestep: 28\n",
            "loss: 1053.92919921875\n",
            "timestep: 29\n",
            "loss: 968.8522338867188\n",
            "timestep: 30\n",
            "loss: 634.2840576171875\n",
            "timestep: 31\n",
            "loss: 673.2420043945312\n",
            "timestep: 32\n",
            "loss: 564.3696899414062\n",
            "timestep: 33\n",
            "loss: 439.34796142578125\n",
            "timestep: 34\n",
            "loss: 376.5731506347656\n",
            "timestep: 35\n",
            "loss: 301.12823486328125\n",
            "timestep: 36\n",
            "loss: 273.3916015625\n",
            "timestep: 37\n",
            "loss: 219.5697021484375\n",
            "timestep: 38\n",
            "loss: 178.56724548339844\n",
            "timestep: 39\n",
            "loss: 161.2665557861328\n",
            "timestep: 40\n",
            "loss: 140.64955139160156\n",
            "timestep: 41\n",
            "loss: 134.05648803710938\n",
            "timestep: 42\n",
            "loss: 116.25827026367188\n",
            "timestep: 43\n",
            "loss: 109.43423461914062\n",
            "timestep: 44\n",
            "loss: 105.67540740966797\n",
            "timestep: 45\n",
            "loss: 89.01715850830078\n",
            "timestep: 46\n",
            "loss: 86.35580444335938\n",
            "timestep: 47\n",
            "loss: 73.92735290527344\n",
            "timestep: 48\n",
            "loss: 85.23500061035156\n",
            "timestep: 49\n",
            "loss: 71.01182556152344\n",
            "timestep: 50\n",
            "loss: 66.51712036132812\n",
            "timestep: 51\n",
            "loss: 61.82582092285156\n",
            "timestep: 52\n",
            "loss: 60.298377990722656\n",
            "timestep: 53\n",
            "loss: 61.286231994628906\n",
            "timestep: 54\n",
            "loss: 57.68136978149414\n",
            "timestep: 55\n",
            "loss: 52.521339416503906\n",
            "timestep: 56\n",
            "loss: 52.10859680175781\n",
            "timestep: 57\n",
            "loss: 58.786468505859375\n",
            "timestep: 58\n",
            "loss: 50.53529357910156\n",
            "timestep: 59\n",
            "loss: 52.206077575683594\n",
            "timestep: 60\n",
            "loss: 52.095664978027344\n",
            "timestep: 61\n",
            "loss: 47.96828079223633\n",
            "timestep: 62\n",
            "loss: 53.96424865722656\n",
            "timestep: 63\n",
            "loss: 51.99456024169922\n",
            "timestep: 64\n",
            "loss: 51.83586502075195\n",
            "timestep: 65\n",
            "loss: 49.18053436279297\n",
            "timestep: 66\n",
            "loss: 56.316383361816406\n",
            "timestep: 67\n",
            "loss: 47.73829650878906\n",
            "timestep: 68\n",
            "loss: 53.71553039550781\n",
            "timestep: 69\n",
            "loss: 55.73973846435547\n",
            "timestep: 70\n",
            "loss: 61.979827880859375\n",
            "timestep: 71\n",
            "loss: 60.24437713623047\n",
            "timestep: 72\n",
            "loss: 60.038185119628906\n",
            "timestep: 73\n",
            "loss: 72.01010131835938\n",
            "timestep: 74\n",
            "loss: 61.46476364135742\n",
            "timestep: 75\n",
            "loss: 54.6983528137207\n",
            "timestep: 76\n",
            "loss: 62.682437896728516\n",
            "timestep: 77\n",
            "loss: 56.80956268310547\n",
            "timestep: 78\n",
            "loss: 49.486717224121094\n",
            "timestep: 79\n",
            "loss: 56.02071762084961\n",
            "timestep: 80\n",
            "loss: 65.22886657714844\n",
            "timestep: 81\n",
            "loss: 57.34381103515625\n",
            "timestep: 82\n",
            "loss: 64.6093521118164\n",
            "timestep: 83\n",
            "loss: 64.82543182373047\n",
            "timestep: 84\n",
            "loss: 68.41084289550781\n",
            "timestep: 85\n",
            "loss: 56.83428192138672\n",
            "timestep: 86\n",
            "loss: 58.579833984375\n",
            "timestep: 87\n",
            "loss: 42.05301284790039\n",
            "timestep: 88\n",
            "loss: 34.04865646362305\n",
            "timestep: 89\n",
            "loss: 43.15987777709961\n",
            "timestep: 90\n",
            "loss: 25.916595458984375\n",
            "timestep: 91\n",
            "loss: 30.25196075439453\n",
            "timestep: 92\n",
            "loss: 25.73451805114746\n",
            "timestep: 93\n",
            "loss: 30.01424789428711\n",
            "timestep: 94\n",
            "loss: 28.015419006347656\n",
            "timestep: 95\n",
            "loss: 24.99555015563965\n",
            "timestep: 96\n",
            "loss: 22.128681182861328\n",
            "timestep: 97\n",
            "loss: 22.870403289794922\n",
            "timestep: 98\n",
            "loss: 22.46534538269043\n",
            "timestep: 99\n",
            "loss: 35.031211853027344\n",
            "timestep: 100\n",
            "loss: 24.60834503173828\n",
            "timestep: 101\n",
            "loss: 18.61267852783203\n",
            "timestep: 102\n",
            "loss: 15.473845481872559\n",
            "timestep: 103\n",
            "loss: 16.15335464477539\n",
            "timestep: 104\n",
            "loss: 13.228147506713867\n",
            "timestep: 105\n",
            "loss: 15.104945182800293\n",
            "timestep: 106\n",
            "loss: 14.933921813964844\n",
            "timestep: 107\n",
            "loss: 13.257301330566406\n",
            "timestep: 108\n",
            "loss: 12.838823318481445\n",
            "timestep: 109\n",
            "loss: 11.36771011352539\n",
            "timestep: 110\n",
            "loss: 13.094327926635742\n",
            "timestep: 111\n",
            "loss: 10.790624618530273\n",
            "timestep: 112\n",
            "loss: 11.623361587524414\n",
            "timestep: 113\n",
            "loss: 12.471582412719727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTv2VRWVxga_"
      },
      "source": [
        "plt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}